---
title: "Statistical Methods in Finance Final Project"
date: "May 09, 2022"
output: "pdf_document"
---

This document is for calculation purposes only and does not represent the final analysis.
```{r, echo = F, eval = F}
rm(list = ls())
```
### Data Processing
```{r, warning=FALSE, message=FALSE}
require(tidyverse)
require(tidyquant)
require(xts)
require(lubridate)
require(tsibble) # Time series tibble

require(MASS) # for fitdistr() and kde2d() functions
require(copula) # for copula functions
require(fGarch) # for standardized t density
require(e1071)
require(nortest)
require(tseries)

require(ggplot2)
require(ggpubr)
require(GGally)
require(png)
require(ggfortify) # for PCA  and factor analysis

theme_set(theme_bw())
```

```{r}
FILENAME = "./data/portfolio_historical_data.csv"
IMG_PATH = "./images/"
DOWNLOAD.DATA = !file.exists(FILENAME)

# Asset symbols that will be used for this analysis
STOCK.SYMBOLS = c(
    "AMD", "MSFT", "SBUX", "AAPL",
    "ITUB", "FB", "NVDA", "F",
    "BAC", "T", "XOM", "VALE"
)
ASSET.NAMES = c(
    "AMD", "Microsoft", "Starbux", "Apple Inc",
    "Itau Unibanco Holding S.A.", "Meta Platforms Inc", "NVIDIA", "Ford",
    "Bank Of America Corp.", "AT&T Inc.", "Exxon Mobil Corp.", "Vale S.A.",
    "S&P 500"
)
SP500.SYMBOL = "GSPC"

YEARS = 5
to.date = lubridate::date("2021-11-1")
from.date = to.date - years(YEARS) - months(1)
dates = seq(from.date, to.date - months(1), by = "months")

risk.free = 0.03
```

```{r}
if(DOWNLOAD.DATA) {
    # Download the assets' hisotrical data and load the
    # variables to the environment
    getSymbols(
        Symbols = c(STOCK.SYMBOLS, paste0("^", SP500.SYMBOL)),
        src = "yahoo",
        from = from.date,
        to = to.date,
        periodicity = "monthly"
    ) %>% suppressMessages()

    historical.data = cbind(
        AMD, MSFT, SBUX, AAPL,
        ITUB, FB, NVDA, F,
        BAC, T, XOM, VALE, GSPC
    )

    # Save the net returns into a csv file
    write.zoo(
        historical.data, FILENAME,
         index.name = "Date", sep = ","
    )
} else {
    historical.data = read.csv.zoo(FILENAME, header = T) %>%
                        xts()
}

adj.columns = endsWith(colnames(historical.data), "Adjusted")
adj.price = historical.data[-1, adj.columns]

colnames(adj.price) = gsub(".Adjusted", "", colnames(adj.price))

# Calcualte the net returns for each asset
# (note: ugly, but gets the work done)
net.returns = cbind(
    CalculateReturns(adj.price[, 1 ], method = "simple"),
    CalculateReturns(adj.price[, 2 ], method = "simple"),
    CalculateReturns(adj.price[, 3 ], method = "simple"),
    CalculateReturns(adj.price[, 4 ], method = "simple"),
    CalculateReturns(adj.price[, 5 ], method = "simple"),
    CalculateReturns(adj.price[, 6 ], method = "simple"),
    CalculateReturns(adj.price[, 7 ], method = "simple"),
    CalculateReturns(adj.price[, 8 ], method = "simple"),
    CalculateReturns(adj.price[, 9 ], method = "simple"),
    CalculateReturns(adj.price[, 10], method = "simple"),
    CalculateReturns(adj.price[, 11], method = "simple"),
    CalculateReturns(adj.price[, 12], method = "simple"),
    CalculateReturns(adj.price[, 13], method = "simple")
)[-1]

sp500.adj = adj.price$GSPC
sp500.returns = net.returns$GSPC

# Remove S&P500 from the adjusted price and net returns data frames
adj.price$GSPC = NULL
net.returns$GSPC = NULL

```

```{r, eval = F, echo = F}
head(historical.data)[, 1:5]
head(adj.price)[, 1:5]
head(net.returns)[, 1:5]
```

## Summary

## Descriptive Statistics

### Sample Statistics
```{r}
returns.summary = summary(net.returns)

# Means
m = colMeans(net.returns)
m

# Standard Deviations
cov.mat = cov(net.returns)
std.dev = diag(cov.mat) %>% sqrt()
std.dev

# Skewness Coefficients
skewness.coeff = skewness(net.returns)
skewness.coeff

# Kurtosis Coefficients
kurtosis.coeff = kurtosis(net.returns) + 3
kurtosis.coeff
```
```{r, echo = F}
# This is a helper function to create plots on a n x m grid given the passed dataset
create.line.plots = function(data, plot.type, x.lab = "x", y.lab = "y", nrow = 4, ncol = 3) {
    n = dim(data)[1]; p = dim(data)[2]
    

    plots = list()
    plot.data = as_tibble(data)
    plot.data$Date = dates[-1:-(length(dates) - n)]

    i = 1
    for (asset in colnames(plot.data)[1:p]) {
        plots[[i]] = plot.data %>% 
                        ggplot(aes(x = Date, y = .data[[asset]])) +
                            plot.type() +
                            scale_x_date() +
                            labs(
                                title = colnames(plot.data)[i],
                                x = x.lab,
                                y = y.lab
                            )
        i = i + 1
    }
    if(p > 1) {
        return(ggarrange(plotlist = plots, nrow = nrow, ncol = ncol))
    }
    return(plots[1])
}

create.histogram = function(data, x.lab = "x", y.lab = "y", nrow = 4, ncol = 3) {
    n = dim(data)[1]; p = dim(data)[2]

    plots = list()
    plot.data = as_tibble(data)

    i = 1
    for (asset in colnames(plot.data)[1:p]) {
        plots[[i]] = plot.data %>% 
                        ggplot(aes(x = .data[[asset]])) +
                            geom_histogram() +
                            labs(
                                title = ASSET.NAMES[i],
                                x = x.lab,
                                y = y.lab
                            )
        i = i + 1
    }
    if(p > 1) {
        return(
            ggarrange(plotlist = plots, nrow = nrow, ncol = ncol) %>%
                suppressMessages()
        )
    }
    return(plots[1] %>% suppressMessages())
}

create.boxplot = function(data, x.lab = "x", y.lab = "y", nrow = 4, ncol = 3) {
    n = dim(data)[1]; p = dim(data)[2]

    plots = list()
    plot.data = as_tibble(data)
    plot.data$Date = dates[-1:-(length(dates) - n)]

    i = 1
    for (asset in colnames(plot.data)[1:p]) {
        plots[[i]] = plot.data %>% 
                        ggplot(aes(y = .data[[asset]])) +
                            geom_boxplot()+
                            labs(
                                title = ASSET.NAMES[i],
                                x = x.lab,
                                y = y.lab
                            )
        i = i + 1
    }
    if(p > 1) {
        return(
            ggarrange(plotlist = plots, nrow = nrow, ncol = ncol) %>%
                suppressMessages()
        )
    }
    return(plots[1] %>% suppressMessages())
}

create.qq.plots = function(data, y.lab = "y", nrow = 4, ncol = 3) {
    n = dim(data)[1]; p = dim(data)[2]

    plots = list()
    plot.data = as_tibble(data)

    i = 1
    for (asset in colnames(plot.data)[1:p]) {
        plots[[i]] = plot.data %>% 
                        ggplot(aes(sample = .data[[asset]])) +
                            stat_qq() +
                            stat_qq_line(color = "red") +
                            labs(
                                title = paste(
                                    ASSET.NAMES[i],
                                    "Normal Q-Q Plot"
                                ),
                                x = "Normal Distribution",
                                y = y.lab
                            ) 
        i = i + 1
    }
    if(p > 1) {
        return(
            ggarrange(plotlist = plots, nrow = nrow, ncol = ncol) %>%
                suppressMessages()
        )
    }
    return(plots[1] %>% suppressMessages())
}

create.equity.curve = function(data, x.lab = x, y.lab = "y") {
    n = dim(data)[1]; p = dim(data)[2]
    dollar.growth = rep(list(rep(1, n + 1)), p)

    for(j in 1:p) {
        s = 1
        m = net.returns[, j] %>% as.numeric()
        for(i in seq.int(1, n)) {
            gr = 1 + m[i]                # gross return
            s = s * gr
            dollar.growth[[j]][i + 1] = s
        }
    }

    names(dollar.growth) = colnames(data)
    dollar.growth = dollar.growth %>% as_tibble()
    create.line.plots(
        dollar.growth, geom_line,
        x.lab = x.lab,
        y.lab = y.lab, nrow = 4, ncol = 3
    )
}
```
### Monthly prices and returns plot
```{r, fig.align="center", fig.width = 12, fig.height=10}
# Yearly Adjusted Price plot
create.line.plots(adj.price, geom_line, x.lab = "Year", y.lab = "Adj. Price")

# Yearly Net Returns plot
create.line.plots(net.returns, geom_area, x.lab = "Year", y.lab = "Net Returns")
# TODO: Check for anormalities
```
```{r, fig.align="center", fig.width = 12, fig.height=10}
create.equity.curve(net.returns, x.lab = "Year", y.lab = "Growth of $1")
create.equity.curve(sp500.returns, x.lab = "Year", y.lab = "Growth of $1")
```
```{r, fig.align="center", fig.width = 12, fig.height=10}
# Yearly Net Returns plot
create.histogram(net.returns,  x.lab = "Net Returns")
create.boxplot(net.returns, y.lab = "Net Returns")
create.qq.plots(net.returns, y.lab = "Net Returns")
# TODO: Check for anormalities
```


### Stationarity test
```{r}
apply(net.returns, 2, adf.test) %>% suppressWarnings()
```
Starbucks and NVIDIA are non-stationary. The rest is stationary

### Normally distributed
```{r}
apply(net.returns, 2, lillie.test)
```
The P-values are all greater than 0.05, hence we don't have
any evidence suggesting that the returns are normally distributed
### Outliers
```{r}
summary(net.returns)
```
The boxplots above suggest the presence of outliers in the dataset. This claim
is further backed by the summary table above.

### Which distribution fits best
```{r}
measure_fitness = function(y, fun, ...) {
    fit = fun(y, ...) %>% suppressWarnings()
    if ("minimum" %in% names(fit)){
        nll = fit$minimum
        p = 3
    } else {
        nll = fit$objective
        p = length(fit$par)
    }
    n = length(y)
    aic = 2 * nll + 2 * p
    bic = 2 * nll - nll * p

    return(c(aic, bic))
}

distributions = c("Standardized t", "Skewed t", "GED", "Skewed GED")
best_fit = function(return) {
    print(paste("Testing:", colnames(return)))
    fitness = data.frame(list(
        std_t = measure_fitness(return, stdFit),
        skewed_std_t = measure_fitness(return, sstdFit),
        ged = measure_fitness(return, gedFit),
        skewed_ged = measure_fitness(return, sgedFit)
    ))
    colnames(fitness) = distributions
    row.names(fitness) = c("AIC", "BIC")

    fitness
}

best_fit(net.returns[, 1 ]) # Standardized t
best_fit(net.returns[, 2 ])
best_fit(net.returns[, 3 ])
best_fit(net.returns[, 4 ])
best_fit(net.returns[, 5 ])
best_fit(net.returns[, 6 ])
best_fit(net.returns[, 7 ])
best_fit(net.returns[, 8 ])
best_fit(net.returns[, 9 ])
best_fit(net.returns[, 10])
best_fit(net.returns[, 11])
best_fit(net.returns[, 12])
```
Every asset except F, T, and XOM (which all three follow a GED distribution)
follows a t distribution

### Sharpe's Slope
```{r}
SharpeRatio(net.returns, RF = risk.free, FUN="StdDev")

```
Microsoft has the highest sharpe slope, meaning that it has
the greatest reward-to-risk ratio over time.

```{r}
# Convert monthly sample means and sd into annual sample means and sd
yearly.sample.means = m * 12
yearly.sample.means

yearly.sample.std.dev = std.dev * sqrt(12)
yearly.sample.std.dev
# Comment on these numbers
```

```{r,fig.align="center", fig.width = 12, fig.height=10, cache = T}
# Construct scatterplots
ggpairs(net.returns)
ggsave(paste0(IMG_PATH, "net.return.scatter.png"), dpi = 700)

# Comment relationships
```

```{r}
# Covariance matrix
cov.mat %>% as_tibble()

# comment on the direction of linear association

```

## Portfolio Theory
```{r}
# Convert the returns into a matrix for easier computations
R = apply(net.returns, 1, function(x) as.numeric(x))

inv.cov.mat = solve(cov.mat)
ones = matrix(1, dim(inv.cov.mat))

# Compute the MVP
w.mvp = (inv.cov.mat %*% ones) / as.numeric(t(ones) %*% inv.cov.mat %*% ones)
w.mvp

# Mean return
m.mvp = t(w.mvp) %*% m %>% as.numeric()
m.mvp

# Standard deviation
var.mvp = t(w.mvp) %*% cov.mat %*% w.mvp %>% as.numeric()
sd.mvp = sqrt(var.mvp)
sd.mvp

R.mvp = apply(
    R, 2, function(x) {
        t(w.mvp) %*% as.matrix(x, ncol = 1) %>% as.numeric()
    }
) %>% as.numeric()


# VaR
S0 = 1e5
alpha.mvp = 0.05
q.mvp = quantile(R.mvp, probs = c(alpha.mvp))
VaR.mvp = -S0 * q.mvp
VaR.mvp

# Expected Shortfall
ES.mvp = sum((-S0 * R) * (-S0 * R > VaR.mvp)) /  sum(-S0 * R > VaR.mvp)


# Comment on the weights
```
```{r}
yearly.m.mvp = m.mvp * 12
yearly.m.mvp

yearly.sd.mvp = sd.mvp * sqrt(12)
yearly.sd.mvp
```

## Asset Allocation
```{r}
target.expected.return.yearly = 0.06
target.expected.return.monthly = target.expected.return.yearly / 12

```

## Principal Component Analysis

Compute the sample correlation matrix of the returns on your assets. 
Which assets are
most highly correlated?
Which are least correlated?

Based on the estimated correlation
values do you think diversification will reduce risk with these assets?

Run the PCA analysis and comment on your results.

Run factor analysis and report the number and the loadings
of each factors. 
Do they have any meaningful interpretation?


```{r}

cor.mat <- cor(net.returns) %>%
    as.data.frame()
           

           

cor.mat > 0.5 & cor.mat < 1

# HIGHEST CORR VALUES : > 0.5

# MSFT AAPL 0.62058674
# MSFT NVDA  0.58334705
# AAPL FB 0.56612120
# AAPL NVDA 0.57386982
# ITUB VALE 0.58532319
# FB BAC 0.5058026
# F BAC 0.57362077
# F XOM 0.5771675
# BAC XOM 0.7005870


# Negative & low correlation vals

cor.mat > 0 - 0.1 & cor.mat < 0 + 0.1


# LOW CORRELATION VALS


# F AMD
# MSFT ITUB

#SBUX NVDA

# AAPL ITUB

# NVDA ITUB

pca = prcomp(net.returns, scale = TRUE)
autoplot(pca, data=net.returns, loadings=TRUE,
         loadings.label=TRUE)


for (i in 1 : 7){
  
  ans  = factanal(x = net.returns, factors = i)

  print(ans)
}


```

## Risk Management

Assume that you have $100,000 to invest. For each asset, estimate the 5% value-at-risk of
the and expected shortfall on $100,000 investment over a one month investment horizon
based on the normal distribution using the estimated means and variances of your assets.
Do the same using the nonparametric method we discussed in class. Which assets have the
highest and lowest VaR at a one month horizon? Which assets have the highest and lowest
expected shortfall at a one month horizon? Do the same for all your portfolios. Use the
bootstrap to compute estimated standard errors and 95% confidence intervals for your 5%
VaR and expected short fall. .


```{r}

```

## Copulas

Use copulas to model the joint distribution of the returns. Which copula fits better the
data? What are the implications?

```{r}

msft <- net.returns[,1] %>% as.numeric()
amd <-  net.returns[,2] %>% as.numeric()
aapl <- net.returns[,3] %>% as.numeric()
sbux <- net.returns[,4] %>% as.numeric()
itub<-  net.returns[,5] %>% as.numeric()
fb <-   net.returns[,6] %>% as.numeric()
nvda <- net.returns[,7] %>% as.numeric()
f <-    net.returns[,8] %>% as.numeric()
bac <-  net.returns[,9] %>% as.numeric()
t <-    net.returns[,10] %>% as.numeric()
xom <-  net.returns[,11] %>% as.numeric()
vale <- net.returns[,12] %>% as.numeric()



est.AMD = as.numeric( fitdistr(amd,"t")$estimate )
est.SBUX = as.numeric( fitdistr(sbux,"t")$estimate )
est.AAPL = as.numeric( fitdistr(aapl,"t")$estimate )
est.MSFT= as.numeric( fitdistr(msft,"t")$estimate )
est.ITUB = as.numeric( fitdistr(itub,"t")$estimate )
est.FB = as.numeric( fitdistr(fb,"t")$estimate )
est.NVDA = as.numeric( fitdistr(nvda,"t")$estimate )
est.F =  as.numeric( fitdistr(f,"t")$estimate )
est.BAC = as.numeric( fitdistr(bac,"t")$estimate )
est.T = as.numeric( fitdistr(t, "t")$estimate )
est.XOM = as.numeric( fitdistr(xom,"t")$estimate )
est.VALE= as.numeric( fitdistr(vale,"t")$estimate )



est.AMD[2] = est.AMD[2] * sqrt( est.AMD[3] / (est.AMD[3]-2) )
est.SBUX[2] = est.SBUX[2] * sqrt(est.SBUX[3] / (est.SBUX[3]-2) )
est.AAPL[2] = est.AAPL[2] * sqrt( est.AAPL[3] / (est.AAPL[3]-2) )
est.MSFT[2] = est.MSFT[2] * sqrt(est.MSFT[3] / (est.MSFT[3]-2) )
est.ITUB[2] = est.ITUB[2] * sqrt( est.ITUB[3] / (est.ITUB[3]-2) )
est.FB[2] = est.FB[2] * sqrt(est.FB[3] / (est.FB[3]-2) )
est.NVDA[2] = est.NVDA[2] * sqrt( est.NVDA[3] / (est.NVDA[3]-2) )
est.F[2] = est.F[2] * sqrt(est.F[3] / (est.F[3]-2) )
est.BAC[2] = est.BAC[2] * sqrt( est.BAC[3] / (est.BAC[3]-2) )
est.T[2] = est.T[2] * sqrt(est.T[3] / (est.T[3]-2) )
est.XOM[2] = est.XOM[2] * sqrt( est.XOM[3] / (est.XOM[3]-2) )
est.VALE[2] = est.VALE[2] * sqrt(est.VALE[3] / (est.VALE[3]-2) )


data1 = cbind(pstd(amd, est.AMD[1], est.AMD[2], est.AMD[3]),
              pstd(sbux, est.SBUX[1], est.SBUX[2], est.SBUX[3]),
              pstd(aapl, est.AAPL[1], est.AAPL[2], est.AAPL[3]),
              pstd(msft, est.MSFT[1], est.MSFT[2], est.MSFT[3]),
              pstd(itub, est.ITUB[1], est.ITUB[2], est.ITUB[3]),
              pstd(fb, est.FB[1], est.FB[2], est.FB[3]),
              pstd(nvda, est.NVDA[1], est.NVDA[2], est.NVDA[3]),
              pstd(f, est.F[1], est.F[2], est.F[3]),
              pstd(bac, est.BAC[1], est.BAC[2], est.BAC[3]),
              pstd(t, est.T[1], est.T[2], est.T[3]),
              pstd(xom, est.XOM[1], est.XOM[2], est.XOM[3]),
              pstd(vale, est.VALE[1], est.VALE[2], est.VALE[3]))



n = nrow(data)


fnorm = fitCopula(copula=normalCopula(dim=12),
                  data=data1,
                  method="ml")

ffrank = fitCopula(copula = frankCopula(3, dim = 12),
                   data = data1,
                   method = "ml")


fgumbel = fitCopula(copula = gumbelCopula(3, dim=12),
                    data = data1,
                    method = "ml")

fjoe = fitCopula(copula=joeCopula(2,dim=12),
                 data=data1,
                 method="ml")

ft = fitCopula(copula = tCopula(dim = 12),
                   data = data1,
                   method = "ml")



AICcalc <- function(fcopula){
  

loglik <- summary(fcopula)$loglik

AIC <- -2*loglik + 2 * length(fcopula@estimate)

return(AIC)
  
}

copula_AIC <- tibble(Normal = AICcalc(fnorm),
       Joe = AICcalc(fjoe),
       Frank = AICcalc(ffrank),
       Gumbel = AICcalc(fgumbel),
       `T` = AICcalc(ft)) 

copula_AIC

min(copula_AIC)
```

## Conclusion
```{r}

```