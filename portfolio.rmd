---
title: "Statistical Methods in Finance Final Project"
date: "May 09, 2022"
output: "pdf_document"
---

This document is for calculation purposes only and does not represent the final analysis.
```{r, echo = F, eval = F}
rm(list = ls())
```
### Data Processing
```{r, warning=FALSE, message=FALSE}
require(tidyverse)
require(tidyquant)
require(xts)
require(lubridate)
require(tsibble) # Time series tibble

require(MASS) # for fitdistr() and kde2d() functions
require(copula) # for copula functions
require(fGarch) # for standardized t density
require(e1071)
require(nortest)
require(tseries)

require(ggplot2)
require(ggpubr)
require(GGally)
require(png)
require(ggfortify) # for PCA  and factor analysis
require(introCompFinR)

theme_set(theme_bw())
```

```{r}
FILENAME = "./data/portfolio_historical_data.csv"
IMG_PATH = "./images/"
DOWNLOAD.DATA = !file.exists(FILENAME)

# Asset symbols that will be used for this analysis
STOCK.SYMBOLS = c(
    "AMD", "MSFT", "SBUX", "AAPL",
    "ITUB", "FB", "NVDA", "F",
    "BAC", "T", "XOM", "VALE"
)
ASSET.NAMES = c(
    "AMD", "Microsoft", "Starbux", "Apple Inc",
    "Itau Unibanco Holding S.A.", "Meta Platforms Inc", "NVIDIA", "Ford",
    "Bank Of America Corp.", "AT&T Inc.", "Exxon Mobil Corp.", "Vale S.A.",
    "S&P 500"
)
SP500.SYMBOL = "GSPC"

YEARS = 5
to.date = lubridate::date("2021-11-1")
from.date = to.date - years(YEARS) - months(1)
dates = seq(from.date, to.date - months(1), by = "months")

mf = 0.03
```

```{r}
if(DOWNLOAD.DATA) {
    # Download the assets' hisotrical data and load the
    # variables to the environment
    getSymbols(
        Symbols = c(STOCK.SYMBOLS, paste0("^", SP500.SYMBOL)),
        src = "yahoo",
        from = from.date,
        to = to.date,
        periodicity = "monthly"
    ) %>% suppressMessages()

    historical.data = cbind(
        AMD, MSFT, SBUX, AAPL,
        ITUB, FB, NVDA, F,
        BAC, T, XOM, VALE, GSPC
    )

    # Save the net returns into a csv file
    write.zoo(
        historical.data, FILENAME,
         index.name = "Date", sep = ","
    )
} else {
    historical.data = read.csv.zoo(FILENAME, header = T) %>%
                        xts()
}

adj.columns = endsWith(colnames(historical.data), "Adjusted")
adj.price = historical.data[-1, adj.columns]

colnames(adj.price) = gsub(".Adjusted", "", colnames(adj.price))

# Calcualte the net returns for each asset
# (note: ugly, but gets the work done)
net.returns = cbind(
    CalculateReturns(adj.price[, 1 ], method = "simple"),
    CalculateReturns(adj.price[, 2 ], method = "simple"),
    CalculateReturns(adj.price[, 3 ], method = "simple"),
    CalculateReturns(adj.price[, 4 ], method = "simple"),
    CalculateReturns(adj.price[, 5 ], method = "simple"),
    CalculateReturns(adj.price[, 6 ], method = "simple"),
    CalculateReturns(adj.price[, 7 ], method = "simple"),
    CalculateReturns(adj.price[, 8 ], method = "simple"),
    CalculateReturns(adj.price[, 9 ], method = "simple"),
    CalculateReturns(adj.price[, 10], method = "simple"),
    CalculateReturns(adj.price[, 11], method = "simple"),
    CalculateReturns(adj.price[, 12], method = "simple"),
    CalculateReturns(adj.price[, 13], method = "simple")
)[-1]

sp500.adj = adj.price$GSPC
sp500.returns = net.returns$GSPC

# Remove S&P500 from the adjusted price and net returns data frames
adj.price$GSPC = NULL
net.returns$GSPC = NULL

```

```{r, eval = F, echo = F}
head(historical.data)[, 1:5]
head(adj.price)[, 1:5]
head(net.returns)[, 1:5]
```

## Summary

## Descriptive Statistics

### Sample Statistics
```{r}
returns.summary = summary(net.returns)

# Means
m = colMeans(net.returns)
m

# Standard Deviations
cov.mat = cov(net.returns)
std.dev = diag(cov.mat) %>% sqrt()
std.dev

# Skewness Coefficients
skewness.coeff = apply(net.returns, 2, skewness)
skewness.coeff

# Kurtosis Coefficients
kurtosis.coeff = apply(net.returns, 2, kurtosis) + 3
kurtosis.coeff

cbind(m, std.dev, skewness.coeff, kurtosis.coeff) %>% round(3)

#TODO: Compute betas
```
```{r, echo = F}
# This is a helper function to create plots on a n x m grid given the passed dataset
create.line.plots = function(data, plot.type, x.lab = "x", y.lab = "y", nrow = 4, ncol = 3) {
    n = dim(data)[1]; p = dim(data)[2]
    

    plots = list()
    plot.data = as_tibble(data)
    plot.data$Date = dates[-1:-(length(dates) - n)]

    i = 1
    for (asset in colnames(plot.data)[1:p]) {
        plots[[i]] = plot.data %>% 
                        ggplot(aes(x = Date, y = .data[[asset]])) +
                            plot.type() +
                            scale_x_date() +
                            labs(
                                title = colnames(plot.data)[i],
                                x = x.lab,
                                y = y.lab
                            )
        i = i + 1
    }
    if(p > 1) {
        return(ggarrange(plotlist = plots, nrow = nrow, ncol = ncol))
    }
    return(plots[1])
}

create.histogram = function(data, x.lab = "x", y.lab = "y", nrow = 4, ncol = 3) {
    n = dim(data)[1]; p = dim(data)[2]

    plots = list()
    plot.data = as_tibble(data)

    i = 1
    for (asset in colnames(plot.data)[1:p]) {
        plots[[i]] = plot.data %>% 
                        ggplot(aes(x = .data[[asset]])) +
                            geom_histogram() +
                            labs(
                                title = ASSET.NAMES[i],
                                x = x.lab,
                                y = y.lab
                            )
        i = i + 1
    }
    if(p > 1) {
        return(
            ggarrange(plotlist = plots, nrow = nrow, ncol = ncol) %>%
                suppressMessages()
        )
    }
    return(plots[1] %>% suppressMessages())
}

create.boxplot = function(data, x.lab = "x", y.lab = "y", nrow = 4, ncol = 3) {
    n = dim(data)[1]; p = dim(data)[2]

    plots = list()
    plot.data = as_tibble(data)
    plot.data$Date = dates[-1:-(length(dates) - n)]

    i = 1
    for (asset in colnames(plot.data)[1:p]) {
        plots[[i]] = plot.data %>% 
                        ggplot(aes(y = .data[[asset]])) +
                            geom_boxplot()+
                            labs(
                                title = ASSET.NAMES[i],
                                x = x.lab,
                                y = y.lab
                            )
        i = i + 1
    }
    if(p > 1) {
        return(
            ggarrange(plotlist = plots, nrow = nrow, ncol = ncol) %>%
                suppressMessages()
        )
    }
    return(plots[1] %>% suppressMessages())
}

create.qq.plots = function(data, y.lab = "y", nrow = 4, ncol = 3) {
    n = dim(data)[1]; p = dim(data)[2]

    plots = list()
    plot.data = as_tibble(data)

    i = 1
    for (asset in colnames(plot.data)[1:p]) {
        plots[[i]] = plot.data %>% 
                        ggplot(aes(sample = .data[[asset]])) +
                            stat_qq() +
                            stat_qq_line(color = "red") +
                            labs(
                                title = paste(
                                    ASSET.NAMES[i],
                                    "Normal Q-Q Plot"
                                ),
                                x = "Normal Distribution",
                                y = y.lab
                            ) 
        i = i + 1
    }
    if(p > 1) {
        return(
            ggarrange(plotlist = plots, nrow = nrow, ncol = ncol) %>%
                suppressMessages()
        )
    }
    return(plots[1] %>% suppressMessages())
}

create.equity.curve = function(data, x.lab = x, y.lab = "y") {
    n = dim(data)[1]; p = dim(data)[2]
    dollar.growth = rep(list(rep(1, n + 1)), p)

    for(j in 1:p) {
        s = 1
        m = net.returns[, j] %>% as.numeric()
        for(i in seq.int(1, n)) {
            gr = 1 + m[i]                # gross return
            s = s * gr
            dollar.growth[[j]][i + 1] = s
        }
    }

    names(dollar.growth) = colnames(data)
    dollar.growth = dollar.growth %>% as_tibble()
    create.line.plots(
        dollar.growth, geom_line,
        x.lab = x.lab,
        y.lab = y.lab, nrow = 4, ncol = 3
    )
}
```

```{r}
s = 1
m_gross = net.returns[, 2] %>% as.numeric()
for(i in seq.int(1, 59)) {
    gr = 1 + m_gross[i]                # gross return
    s = s * gr
}
s
```
### Monthly prices and returns plot
```{r, fig.align="center", fig.width = 12, fig.height=10}
# Yearly Adjusted Price plot
create.line.plots(adj.price, geom_line, x.lab = "Year", y.lab = "Adj. Price")

# Yearly Net Returns plot
create.line.plots(net.returns, geom_area, x.lab = "Year", y.lab = "Net Returns")
# TODO: Check for anormalities
```
```{r, fig.align="center", fig.width = 12, fig.height=10}
create.equity.curve(net.returns, x.lab = "Year", y.lab = "Growth of $1")
create.equity.curve(sp500.returns, x.lab = "Year", y.lab = "Growth of $1")
```
```{r, fig.align="center", fig.width = 12, fig.height=10}
# Yearly Net Returns plot
create.histogram(net.returns,  x.lab = "Net Returns")
create.boxplot(net.returns, y.lab = "Net Returns")
create.qq.plots(net.returns, y.lab = "Net Returns")
# TODO: Check for anormalities
```


### Stationarity test
```{r}
apply(net.returns, 2, adf.test) %>% suppressWarnings()
```
Starbucks and NVIDIA are non-stationary. The rest is stationary

### Normally distributed
```{r}
apply(net.returns, 2, lillie.test)
```
The P-values are all greater than 0.05, hence we don't have
any evidence suggesting that the returns are normally distributed
### Outliers
```{r}
summary(net.returns)
```
The boxplots above suggest the presence of outliers in the dataset. This claim
is further backed by the summary table above.

### Which distribution fits best
```{r}
measure_fitness = function(y, fun, ...) {
    fit = fun(y, ...) %>% suppressWarnings()
    if ("minimum" %in% names(fit)){
        nll = fit$minimum
        p = 3
    } else {
        nll = fit$objective
        p = length(fit$par)
    }
    n = length(y)
    aic = 2 * nll + 2 * p
    bic = 2 * nll - nll * p

    return(c(aic, bic))
}

distributions = c("Standardized t", "Skewed t", "GED", "Skewed GED")
best_fit = function(return) {
    print(paste("Testing:", colnames(return)))
    fitness = data.frame(list(
        std_t = measure_fitness(return, stdFit),
        skewed_std_t = measure_fitness(return, sstdFit),
        ged = measure_fitness(return, gedFit),
        skewed_ged = measure_fitness(return, sgedFit)
    ))
    colnames(fitness) = distributions
    row.names(fitness) = c("AIC", "BIC")

    fitness
}

best_fit(net.returns[, 1 ]) # Standardized t
best_fit(net.returns[, 2 ])
best_fit(net.returns[, 3 ])
best_fit(net.returns[, 4 ])
best_fit(net.returns[, 5 ])
best_fit(net.returns[, 6 ])
best_fit(net.returns[, 7 ])
best_fit(net.returns[, 8 ])
best_fit(net.returns[, 9 ])
best_fit(net.returns[, 10])
best_fit(net.returns[, 11])
best_fit(net.returns[, 12])
```
Every asset except F, T, and XOM (which all three follow a GED distribution)
follows a t distribution

### Sharpe's Slope
```{r}
SharpeRatio(net.returns, RF = mf, FUN="StdDev")

```
Microsoft has the highest sharpe slope, meaning that it has
the greatest reward-to-risk ratio over time.

```{r}
# Convert monthly sample means and sd into annual sample means and sd
yearly.sample.means = m * 12
yearly.sample.means

yearly.sample.std.dev = std.dev * sqrt(12)
yearly.sample.std.dev
# Comment on these numbers
```

```{r,fig.align="center", fig.width = 12, fig.height=10, cache = T}
# Construct scatterplots
ggpairs(net.returns)
ggsave(paste0(IMG_PATH, "net.return.scatter.png"), dpi = 700)

# Comment relationships
```

```{r}
# Covariance matrix
cov.mat %>% round(5)
cov.mat
# comment on the direction of linear association

```

## Portfolio Theory
```{r}
# Convert the returns into a matrix for easier computations
R = apply(net.returns, 1, function(x) as.numeric(x))

inv.cov.mat = solve(cov.mat)
ones = matrix(1, dim(inv.cov.mat))

# Compute the MVP
w.mvp = (inv.cov.mat %*% ones) / as.numeric(t(ones) %*% inv.cov.mat %*% ones)
w.mvp

# Mean return
m.mvp = t(w.mvp) %*% m %>% as.numeric()
m.mvp

# Standard deviation
var.mvp = t(w.mvp) %*% cov.mat %*% w.mvp %>% as.numeric()
sd.mvp = sqrt(var.mvp)
sd.mvp

# Comment on the weights
```

### Yearly MVP mean and sd
```{r}
yearly.m.mvp = m.mvp * 12
yearly.m.mvp

yearly.sd.mvp = sd.mvp * sqrt(12)
yearly.sd.mvp
# Comment on these values relative to those of each asset
```

### MVP Returns
```{r}
R.mvp = apply(
    R, 2, function(x) {
        t(w.mvp) %*% as.matrix(x, ncol = 1) %>% as.numeric()
    }
) %>% 
as.numeric() %>%
as.matrix(ncol = 1)
R.mvp
```

### VaR and ES (no short-sales)
```{r}
# VaR
S0 = 1e5
alpha = 0.05

q.mvp = quantile(R.mvp, probs = c(alpha))
VaR.mvp = (-S0 * q.mvp) %>% as.numeric()
VaR.mvp

# Expected Shortfall
ES.mvp = sum((-S0 * R) * (-S0 * R > VaR.mvp)) /  sum(-S0 * R > VaR.mvp)
ES.mvp

individual.VaR = VaR(net.returns, p = 1 - alpha, method = "historical") * -S0
individual.VaR
```
### VaR and ES (Short-sales allowed)
```{r}
VaR.mvp.shorting = -VaR.mvp
VaR.mvp.shorting

# Expected Shortfall
ES.mvp.shorting = sum((S0 * R) * (S0 * R > VaR.mvp)) /  sum(S0 * R > VaR.mvp)
ES.mvp.shorting

# Compare the VaR and ES
individual.VaR
```
### Efficient Portfolio (SHORTS ALLOWED)
```{r}
efficient.portfolio = portfolio.optim(net.returns, rf = mf)
ef.filter = efficient.portfolio$pm >= m.mvp


efficient.portfolio.shorting = portfolio.optim(
                                    net.returns,
                                    rf = mf,
                                    shorts = TRUE
                                )
m.eps = efficient.portfolio.shorting$px
sd.eps = efficient.portfolio.shorting$ps
sharpes.eps = (m.eps - mf) / sd.eps
t.eps.filter = sharpes.eps == max(sharpes.eps)
wt.eps = efficient.portfolio.shorting$px[t.eps.filter]

SharpeRatio(eps.returns, Rf = mf, FUN = "StdDev")
w.mvp
efficient.portfolio.shorting$pw

```
### Efficient Portfolio (NO SHORTS)

```{r}
efficient.portfolio = portfolio.optim(net.returns, rf = mf)
ef.filter = efficient.portfolio$pm >= m.mvp


efficient.portfolio.shorting = portfolio.optim(
                                    net.returns,
                                    rf = mf,
                                    shorts = TRUE
                                )
m.eps = efficient.portfolio.shorting$px
sd.eps = efficient.portfolio.shorting$ps
sharpes.eps = (m.eps - mf) / sd.eps
t.eps.filter = sharpes.eps == max(sharpes.eps)
wt.eps = efficient.portfolio.shorting$px[t.eps.filter]

SharpeRatio(eps.returns, Rf = mf, FUN = "StdDev")
w.mvp
efficient.portfolio.shorting$pw

```


### Tangency Portfolio

```{r}

tangency.portfolio <-
function(er,cov.mat,risk.free = rf, shorts=TRUE)
{
  call <- match.call()

  #
  # check for valid inputs
  #
  asset.names <- names(er)
  # if(risk.free < 0){
  #   stop("Risk-free rate must be positive")}
  er <- as.vector(er)
  cov.mat <- as.matrix(cov.mat)
  N <- length(er)
  if(N != nrow(cov.mat)){
    stop("invalid inputs")}
  if(any(diag(chol(cov.mat)) <= 0)){
    stop("Covariance matrix not positive definite")}
  # remark: could use generalized inverse if cov.mat is positive semi-definite

  #
  # compute global minimum variance portfolio
  #
  gmin.port <- globalMin.portfolio(er, cov.mat, shorts=shorts)
  if(gmin.port$er < risk.free){
    stop("Risk-free rate greater than avg return on global minimum variance portfolio")}

  # 
  # compute tangency portfolio
  #
# Tangency portfolio

  if(shorts==TRUE){
    cov.mat.inv <- solve(cov.mat)
    w.t <- cov.mat.inv %*% (er - risk.free) # tangency portfolio
    w.t <- as.vector(w.t/sum(w.t))          # normalize weights
  } else if(shorts==FALSE){
    Dmat <- 2*cov.mat
    dvec <- rep.int(0, N)
    er.excess <- er - risk.free
    Amat <- cbind(er.excess, diag(1,N))
    bvec <- c(1, rep(0,N))
    result <- quadprog::solve.QP(Dmat=Dmat,dvec=dvec,Amat=Amat,bvec=bvec,meq=1)
    w.t <- round(result$solution/sum(result$solution), 6)
  } else {
    stop("Shorts needs to be logical. For no-shorts, shorts=FALSE.")
  }
    
  names(w.t) <- asset.names
  er.t <- crossprod(w.t,er)
  sd.t <- sqrt(t(w.t) %*% cov.mat %*% w.t)
  tan.port <- list("call" = call,
		   "er" = as.vector(er.t),
		   "sd" = as.vector(sd.t),
		   "weights" = w.t)
  class(tan.port) <- "portfolio"
  return(tan.port)
}

tangency.portfolio(m,cov.mat,rf,shorts = FALSE)
```

```{r}

er <- m
asset.names <- names(er)
# if(risk.free < 0){
#   stop("Risk-free rate must be positive")}
er <- as.vector(er)
cov.mat <- as.matrix(cov.mat)
N <- nrow(cov.mat)
  
Dmat <- 2*cov.mat
dvec <- rep.int(0, N)
er.excess <- er - risk.free
#Amat <- cbind(er.excess, diag(1,N))
Amat <- cbind(rep(1,N), m)
bvec <- c(1, .10)
result <- quadprog::solve.QP(Dmat=Dmat,dvec=dvec,Amat=Amat,bvec=bvec,meq=1)
w.t <- round(result$solution/sum(result$solution), 6)
  
w.t  
# WITHOUT SHORT SALES

    
# w.t = tangency.portfolio(er = R,
#                          cov.mat = cov.mat,
#                          risk.free = rf,
#                          shorts = FALSE)
# w.t = inv.cov.mat %*% (m - mf * ones) / ((t(ones) %*% inv.cov.mat %*% (m - mf * ones))[1])
# w.t1 = w.t
```

## Asset Allocation

### Only Risky Assets Allocation
Suppose you wanted to achieve a target expected return of 6% per year (which corresponds
to an expected return of 0.5% per month) using only the risky assets and no short sales
allowed, what is the efficient portfolio that achieves this target return? How much is invested
in each of the assets in this efficient portfolio?

Compute:
monthly risk on this efficient portfolio


```{r}
target.expected.return.yearly = 0.06
target.expected.return.monthly = target.expected.return.yearly / 12

eff.port.return.risky = portfolio.optim(x = net.returns,
                                        pm = target.expected.return.monthly)

R.eff.port.risky = eff.port.return.risky$px

print(paste("The monthly risl on this efficient portfolio is ", round(eff.port.return.risky$ps, 4)))



```

Compute monthly 5% value-at-risk and expected shortfall based on an initial $100,000 investment.
```{r}
# Var and ES

# VaR
S0 = 1e5
alpha = 0.05

q.eff.port.risky = quantile(R.eff.port.risky,
                            probs = c(alpha))
VaR.eff.port.risky = (-S0 * q.eff.port.risky) %>% as.numeric()

VaR.eff.port.risky

# Expected Shortfall
ES.eff.port.risky = sum((-S0 * R) * (-S0 * R > VaR.eff.port.risky)) /  sum(-S0 * R > VaR.eff.port.risky)
ES.eff.port.risky

individual.VaR.eff.port.risky = VaR(net.returns, p = 1 - alpha, method = "historical") * -S0
individual.VaR.eff.port.risky
```



### T Bills + Tangency Portfolio Asset Allocation


Now suppose you wanted to achieve a target expected return of 6%
per year (which corresponds to an expected return of 0.5% per month) using a combination
of T-Bills and the tangency portfolio (that does not allow for short sales)
In this allocation,
how much is invested in each of the assets and how much is invested in the risk free asset?

```{r}
# W*mean_T + 1-w(rf) = 0.05
```

Compute the monthly risk on this efficient portfolio, as well as the monthly and 5% value-
at-risk and expected shortfall based on an initial $100,000 investment.

Compare this with the VaR computed from the allocation of risky assets without short sales.



## Principal Component Analysis


Run factor analysis and report the number and the loadings
of each factors. 
Do they have any meaningful interpretation? -->



```{r}

cor.mat <- cor(net.returns) %>%
    as.data.frame()

           
cor.mat
           


cor.mat > 0.5 & cor.mat < 1

# HIGHEST CORR VALUES : > 0.5

# MSFT AAPL 0.62058674
# MSFT NVDA  0.58334705
# AAPL FB 0.56612120
# AAPL NVDA 0.57386982

# ITUB VALE 0.58532319

# FB BAC 0.5058026

# F BAC 0.57362077
# F XOM 0.5771675
# BAC XOM 0.7005870


# Negative & low correlation vals

cor.mat > 0 - 0.1 & cor.mat < 0 + 0.1


# LOW CORRELATION VALS


# F AMD
# MSFT ITUB

#SBUX NVDA

# AAPL ITUB

# NVDA ITUB




```
Compute the sample correlation matrix of the returns on your assets. 
Which assets are

most highly correlated?

The most highly correlated assets by correlation coefficient were:


### BAC XOM 0.7005870
### MSFT AAPL 0.62058674
### ITUB VALE 0.58532319
### MSFT NVDA  0.58334705
### F XOM 0.5771675
### AAPL NVDA 0.57386982
### F BAC 0.57362077
### AAPL FB 0.56612120
### FB BAC 0.5058026

Of these top 9 highest most correlated stocks, 4 were purely between technology stocks:
MSFT AAPL
MSFT NVDA
NVDA AAPL
AAPL FB

4 were between non-tech sector stocks
BAC XOM
ITUB VALE
F XOM
F BAC

1 was between a tech and a non tech sector stock.
FB BAC


Which are least correlated?

The following 5 stock combinations had the lowest correlation numbers.

### NVDA ITUB  -0.06996619

### MSFT ITUB 0.02825463

### AAPL ITUB  0.03429805

### NVDA SBUX 0.06270411

### F AMD 0.06624342

From these results we can see that all of these stock combinations of low correlations are between a technology sector stock and a non-technology sector stock.

The high (but no so high) level of correlation between both tech stocks among themselves and non tech sector stocks among themselves in addition to the low correlation numbers between tech and non tech stocks prove that the diversification and choice of assets in this portfolio serve the purpose of reducing the overall risk.

```{r}
pca = prcomp(net.returns, scale = TRUE)

autoplot(pca, data=net.returns, loadings=TRUE,
         loadings.label=TRUE,
         x = 1,
         y =2)

autoplot(pca, data=net.returns, loadings=TRUE,
         loadings.label=TRUE,
         x = 2,
         y =3)


```

Run the PCA analysis and comment on your results.

From the plot of this principal component analysis, we can observe two distinct groups of which these eigenvectors(arrows) point in when plotting PC1 against PC2, and 3 distinct groups when plotting PC2 against PC3. Each time, however, the technology sector stocks of NVDA,AAPL,MSFT,FB,AMD are grouped together, while the other seven are grouped together in the 1st plot and split into (SBUX, T, BAC, F, and XOM) and (ITUB and VALE) in the second one.




```{r}
for (i in 1 : 7){
  
  ans  = factanal(x = net.returns, factors = i, scores = "regression")
  

  print(ans$loadings)
  
}

ans  = factanal(x = net.returns, factors = 2, scores = "regression")



autoplot(ans, net.returns,
         labels = TRUE,
         loadings = TRUE,
         loadings.label = TRUE)
```



Do they have any meaningful interpretation?

From our factor analysis we can observe from our graph of factor 1 vs factor 2 that there is a clear distinction between the two groups of technology vs non technology stocks. Moreover, the p-value of the hypothesis that 2 factors were sufficient is statistically significant in explaining our findings. In other words, our data having two factors makes sense visually and is backed up by the statistical tests run.



## Risk Management

Assume that you have $100,000 to invest. For each asset, estimate the 5% value-at-risk of
the and expected shortfall on $100,000 investment over a one month investment horizon
based on the normal distribution using the estimated means and variances of your assets.

Which assets have the
highest and lowest VaR at a one month horizon? 

Which assets have the highest and lowest
expected shortfall at a one month horizon?

Do the same using the nonparametric method we discussed in class. 

Do the same for all your portfolios. (Efficient, MVP, Efficient Rsky, tangency + risk free )

Use the
bootstrap to compute estimated standard errors and 95% confidence intervals for your 5%
VaR and expected short fall. .

### Normal parametric VaR and ES

```{r}
alpha <- 0.05
S0 <- 1e5

vaR.norm <- -S0*VaR(net.returns, p = 1- alpha, method="gaussian")

vaR.norm

max.VaR = max(vaR.norm) # AMD

print(paste0("Max VaR is for AMD stock = ", round(max.VaR,3) ))

min.VaR = min(vaR.norm) # MSFT

print(paste0("Min VaR is for MSFT stock = ", round(min.VaR,3) ))

ES.norm = -S0*ES(net.returns, p = 1-alpha, method = "gaussian")
ES.norm

max.ES = max(ES.norm) # AMD

print(paste0("Max ES is for AMD stock = ", round(max.ES,3) ))

min.ES = min(ES.norm) # MSFT

print(paste0("Min ES is for MSFT stock = ", round(min.ES,3) ))




```

### Nonparametric VaR and ES

```{r}
# VaR
S0 = 1e5
alpha = 0.05
R.nonparam = R
q.nonparam = quantile(R.nonparam, probs = c(alpha))
  VaR.nonparam = (-S0 * q.nonparam) %>% as.numeric()
VaR.nonparam

# Expected Shortfall
ES.nonparam = sum((-S0 * R) * (-S0 * R > VaR.nonparam)) /  sum(-S0 * R > VaR.nonparam)
ES.nonparam

individual.VaR = VaR(net.returns, p = 1 - alpha, method = "historical") * -S0
individual.VaR

# Nonparametric VaR



# Nonparametric ES

```
### Bootstrapping

```{r}
require(rugarch)
garch.norm <- ugarchspec(mean.model = list(armaOrder = c(0,0)),
                         variance.model = list(garchOrder = c(1,1)),
                         distribution.model = "norm")
sp.garch.norm <- ugarchfit(data=net.returns,spec=garch.norm)

pred = ugarchforecast(sp.garch.norm, data = net.returns, n.ahead = 1)
pred
```

## Copulas

Use copulas to model the joint distribution of the returns. Which copula fits better the
data? What are the implications?

```{r}

msft <- net.returns[,1] %>% as.numeric()
amd <-  net.returns[,2] %>% as.numeric()
aapl <- net.returns[,3] %>% as.numeric()
sbux <- net.returns[,4] %>% as.numeric()
itub<-  net.returns[,5] %>% as.numeric()
fb <-   net.returns[,6] %>% as.numeric()
nvda <- net.returns[,7] %>% as.numeric()
f <-    net.returns[,8] %>% as.numeric()
bac <-  net.returns[,9] %>% as.numeric()
t <-    net.returns[,10] %>% as.numeric()
xom <-  net.returns[,11] %>% as.numeric()
vale <- net.returns[,12] %>% as.numeric()



est.AMD = as.numeric( fitdistr(amd,"t")$estimate )
est.SBUX = as.numeric( fitdistr(sbux,"t")$estimate )
est.AAPL = as.numeric( fitdistr(aapl,"t")$estimate )
est.MSFT= as.numeric( fitdistr(msft,"t")$estimate )
est.ITUB = as.numeric( fitdistr(itub,"t")$estimate )
est.FB = as.numeric( fitdistr(fb,"t")$estimate )
est.NVDA = as.numeric( fitdistr(nvda,"t")$estimate )
est.F =  as.numeric( fitdistr(f,"t")$estimate )
est.BAC = as.numeric( fitdistr(bac,"t")$estimate )
est.T = as.numeric( fitdistr(t, "t")$estimate )
est.XOM = as.numeric( fitdistr(xom,"t")$estimate )
est.VALE= as.numeric( fitdistr(vale,"t")$estimate )



est.AMD[2] = est.AMD[2] * sqrt( est.AMD[3] / (est.AMD[3]-2) )
est.SBUX[2] = est.SBUX[2] * sqrt(est.SBUX[3] / (est.SBUX[3]-2) )
est.AAPL[2] = est.AAPL[2] * sqrt( est.AAPL[3] / (est.AAPL[3]-2) )
est.MSFT[2] = est.MSFT[2] * sqrt(est.MSFT[3] / (est.MSFT[3]-2) )
est.ITUB[2] = est.ITUB[2] * sqrt( est.ITUB[3] / (est.ITUB[3]-2) )
est.FB[2] = est.FB[2] * sqrt(est.FB[3] / (est.FB[3]-2) )
est.NVDA[2] = est.NVDA[2] * sqrt( est.NVDA[3] / (est.NVDA[3]-2) )
est.F[2] = est.F[2] * sqrt(est.F[3] / (est.F[3]-2) )
est.BAC[2] = est.BAC[2] * sqrt( est.BAC[3] / (est.BAC[3]-2) )
est.T[2] = est.T[2] * sqrt(est.T[3] / (est.T[3]-2) )
est.XOM[2] = est.XOM[2] * sqrt( est.XOM[3] / (est.XOM[3]-2) )
est.VALE[2] = est.VALE[2] * sqrt(est.VALE[3] / (est.VALE[3]-2) )


data1 = cbind(pstd(amd, est.AMD[1], est.AMD[2], est.AMD[3]),
              pstd(sbux, est.SBUX[1], est.SBUX[2], est.SBUX[3]),
              pstd(aapl, est.AAPL[1], est.AAPL[2], est.AAPL[3]),
              pstd(msft, est.MSFT[1], est.MSFT[2], est.MSFT[3]),
              pstd(itub, est.ITUB[1], est.ITUB[2], est.ITUB[3]),
              pstd(fb, est.FB[1], est.FB[2], est.FB[3]),
              pstd(nvda, est.NVDA[1], est.NVDA[2], est.NVDA[3]),
              pstd(f, est.F[1], est.F[2], est.F[3]),
              pstd(bac, est.BAC[1], est.BAC[2], est.BAC[3]),
              pstd(t, est.T[1], est.T[2], est.T[3]),
              pstd(xom, est.XOM[1], est.XOM[2], est.XOM[3]),
              pstd(vale, est.VALE[1], est.VALE[2], est.VALE[3]))


fnorm = fitCopula(copula=normalCopula(dim=12),
                  data=data1,
                  method="ml")

ffrank = fitCopula(copula = frankCopula(3, dim = 12),
                   data = data1,
                   method = "ml")


fgumbel = fitCopula(copula = gumbelCopula(3, dim=12),
                    data = data1,
                    method = "ml")

fjoe = fitCopula(copula=joeCopula(2,dim=12),
                 data=data1,
                 method="ml")

ft = fitCopula(copula = tCopula(dim = 12),
                   data = data1,
                   method = "ml")



AICcalc <- function(fcopula){
  

loglik <- summary(fcopula)$loglik

AIC <- -2*loglik + 2 * length(fcopula@estimate)

return(AIC)
  
}

copula_AIC <- tibble(Normal = AICcalc(fnorm),
       Joe = AICcalc(fjoe),
       Frank = AICcalc(ffrank),
       Gumbel = AICcalc(fgumbel),
       `T` = AICcalc(ft)) 

copula_AIC

min(copula_AIC)

print(paste("We choose The T Copula as it had the lowest AIC of",min(copula_AIC) ))

```

### Implications?


## Conclusion
```{r}

```